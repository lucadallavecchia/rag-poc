# ðŸ§  Spring Boot RAG PoC (Retrieval-Augmented Generation) with Ollama + Mistral

This is a Proof of Concept (PoC) for a RAG (Retrieval-Augmented Generation) system built with Java 21 and Spring Boot.  
It integrates a local LLM via Ollama (Mistral) with an in-memory vector store and all-MiniLM-L6-v2 embedding model (via Djl), allowing you to ask questions about a specific context.
The context is provided by documents that can be added via URLs, which are then processed to extract their content.

---

## ðŸ“– Overview Apis
- âœ… REST API `POST /ask` to receive user questions
- âœ… REST API `GET /documents` to list all documents
- âœ… REST API `POST /documents` to add a new document by URL

## ðŸ“Œ Features

- âœ… Firecrawler integration to get the markdown of a web page
- âœ… Embedding (Djl with all-MiniLM-L6-v2 model)
- ðŸš§ In-memory vector store (`InMemoryVectorStore`)
- âœ… Local Ollama (Mistral model) integration via Spring AI
- ðŸš§  Automatic text chunking for better semantic retrieval

---

## ðŸ§± Tech Stack

| Technology  | Description                               |
|-------------|-------------------------------------------|
| Java 21     | Programming language                      |
| Spring Boot | Backend framework                         |
| Firecrawl   | Service to retrieve the markdown of a url |
| Spring AI   | For LLM + embeddings integration          |
| Djl         | For embeddings (all-MiniLM-L6-v2)         |
| Ollama      | Local LLM runtime                         |
| Mistral     | LLM model used to generate responses      |

---

## ðŸš€ Project Setup

### ðŸ”§ Requirements

- Java 21
- Gradle (use the wrapper in the codebase)
- Firecrawl API key (for web page retrieval)
- Ollama installed locally with the `mistral` model
